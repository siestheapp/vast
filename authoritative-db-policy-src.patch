--- a/src/conversation.py
+++ b/src/conversation.py
@@
-        system_msg = Message(
+        system_msg = Message(
             role=MessageRole.SYSTEM,
             content=f"""You are VAST, an AI Database Architect and Operator. You are like a senior DBA/CTO who:
@@
-        Tone: precise, confident, and free of pleasantries or invitations (no "let me know" or "feel free to ask"). Respond like a senior engineer briefing peers—short, factual sentences, include only necessary detail.
+        Tone: precise, confident, and free of pleasantries or invitations (no "let me know" or "feel free to ask"). Respond like a senior engineer briefing peers—short, factual sentences, include only necessary detail.
+
+        Authoritative DB Policy:
+        - For any question about the CURRENT database (counts, sizes, existence, schema, relationships, examples), you MUST:
+          1) Generate a read-only SQL query against the connected database,
+          2) Execute it via VAST (no guessing),
+          3) Answer ONLY from the query result. Include a compact Markdown table when helpful.
+        - Never use phrases like “typically”, “usually”, “likely”, or generic answers about databases.
+        - If you cannot run a query, reply: "I need to run a query to determine this." and include the SQL in ```sql fences.
             """
         )
@@
     def process(self, user_input: str, auto_execute: bool = False) -> str:
@@
-        # Clear last actions
+        # Clear last actions
         self.last_actions = []
 
+        # --- Fast paths that MUST be grounded on the live DB -----------------
+        text_lower = user_input.lower()
+        if ("biggest tables" in text_lower) or (("largest" in text_lower) and ("tables" in text_lower)):
+            data = self._biggest_tables(limit=10)
+            md = self._render_biggest_tables_markdown(data)
+            assistant_msg = Message(role=MessageRole.ASSISTANT, content=md)
+            self.messages.append(assistant_msg)
+            self._save_session()
+            return md
+        # ---------------------------------------------------------------------
+
@@
-        # Add assistant response to history
-        assistant_msg = Message(role=MessageRole.ASSISTANT, content=response)
+        # Add assistant response to history (with grounding enforcement)
+        response = self._enforce_grounding(user_input, response)
+        assistant_msg = Message(role=MessageRole.ASSISTANT, content=response)
         self.messages.append(assistant_msg)
@@
         return response
+
+    # ------------------------ Grounded helpers -------------------------------
+    def _biggest_tables(self, limit: int = 10):
+        """Return largest tables by total size using pg_total_relation_size"""
+        from sqlalchemy import text
+        from .db import get_engine  # adjust import path if different in your project
+        sql = """
+        SELECT
+          n.nspname AS schema,
+          c.relname AS table,
+          pg_total_relation_size(c.oid) AS total_bytes,
+          pg_relation_size(c.oid)      AS table_bytes,
+          COALESCE(pg_stat_get_live_tuples(c.oid), 0) AS approx_rows
+        FROM pg_class c
+        JOIN pg_namespace n ON n.oid = c.relnamespace
+        WHERE c.relkind = 'r'
+        ORDER BY pg_total_relation_size(c.oid) DESC
+        LIMIT :limit;
+        """
+        with get_engine(readonly=True).begin() as conn:
+            rows = conn.execute(text(sql), {"limit": limit}).mappings().all()
+        return rows
+
+    def _render_biggest_tables_markdown(self, rows) -> str:
+        def fmt_bytes(b):
+            units = ["B","KB","MB","GB","TB","PB"]
+            i = 0
+            b = float(b or 0)
+            while b >= 1024 and i < len(units)-1:
+                b /= 1024.0
+                i += 1
+            return f"{b:.0f} {units[i]}"
+        lines = [
+            "Here are the largest tables **in the connected database** (by total size):",
+            "",
+            "| # | schema.table | size | rows |",
+            "|---|--------------|------|------|",
+        ]
+        for i, r in enumerate(rows, 1):
+            fq = f"{r['schema']}.{r['table']}"
+            lines.append(f"| {i} | `{fq}` | {fmt_bytes(r['total_bytes'])} | {int(r['approx_rows'])} |")
+        return "\n".join(lines)
+
+    # ---------------------- Grounding enforcement ---------------------------
+    import re as _re  # local alias to avoid name clashes
+    _HEDGES = _re.compile(r"\\b(typically|usually|likely|generally|commonly|often|might)\\b", _re.I)
+    def _is_db_fact_question(self, s: str) -> bool:
+        s = s.lower()
+        return any(k in s for k in [
+            "how many","count","size","largest","biggest","rows","tables","indexes",
+            "what database","connected to","schema","columns","foreign key","table size"
+        ])
+
+    def _enforce_grounding(self, user_input: str, response: str) -> str:
+        """If the user asked a DB-fact question, forbid hedgy language unless we ran SQL."""
+        if not response:
+            return response
+        if self._is_db_fact_question(user_input) and self._HEDGES.search(response):
+            return ("I need to run a query to determine this.\n\n"
+                    "```sql\n-- VAST will generate and execute the appropriate SELECT against the connected database\n```")
+        return response
